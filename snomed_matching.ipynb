{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbac5eaf-20a2-4411-a3fc-2dc0f2144944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "from functools import lru_cache  # For caching\n",
    "import os\n",
    "import ast\n",
    "import subprocess\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# need to get SNOMED files, see https://www.snomed.org/get-snomed\n",
    "# have to start with snomed_concepts, because descriptions have old (inactive) concepts in there that were not updated and are marked as active descriptions\n",
    "snomed_concepts = pd.read_csv(\"../snomed/SnomedCT_InternationalRF2_PRODUCTION_20230331T120000Z/Snapshot/Terminology/sct2_Concept_Snapshot_INT_20230331.txt\", delimiter =\"\\t\")\n",
    "active_snomed_concepts = list(snomed_concepts[snomed_concepts['active']==1]['id'].values)\n",
    "\n",
    "snomed_descriptions = pd.read_csv(\"../snomed/SnomedCT_InternationalRF2_PRODUCTION_20230331T120000Z/Snapshot/Terminology/sct2_Description_Snapshot-en_INT_20230331.txt\", delimiter =\"\\t\")\n",
    "active_snomed_descriptions = snomed_descriptions[(snomed_descriptions['active']==1) & (snomed_descriptions['conceptId'].isin(active_snomed_concepts))]\n",
    "\n",
    "# In SNOMED CT, the preferred term label has the semantic tag in parentheses\n",
    "prefered_id = 900000000000003001\n",
    "\n",
    "# Filter active SNOMED descriptions to get those with the preferred term typeId\n",
    "df_fsn = active_snomed_descriptions[active_snomed_descriptions['typeId'] == prefered_id]\n",
    "\n",
    "def find_in_parentheses(text):\n",
    "    # Regular expression to find all content within parentheses\n",
    "    matches = re.findall(r'\\(([^)]+)\\)', text)\n",
    "    if matches:\n",
    "        return matches[-1]  # Returns the last text within the parentheses\n",
    "    return None\n",
    "\n",
    "# Extract the semantic tag from the term column\n",
    "df_fsn['semantic_tag'] = df_fsn['term'].apply(find_in_parentheses)\n",
    "\n",
    "# Create dictionaries mapping conceptId to term and semantic tag\n",
    "sctid_to_term = df_fsn.set_index('conceptId')['term'].to_dict()\n",
    "sctid_to_tag = df_fsn.set_index('conceptId')['semantic_tag'].to_dict()\n",
    "\n",
    "# Find abbreviations in the term column\n",
    "mask = active_snomed_descriptions['term'].str.match(r'[A-Z]{2,4}\\s-\\s.*', na=False)\n",
    "abbreviations = active_snomed_descriptions[mask].copy()\n",
    "\n",
    "# Extract the abbreviation part of the term\n",
    "abbreviations['term'] = abbreviations['term'].map(lambda x: x.split('-')[0].strip())\n",
    "\n",
    "# Keep only the first occurrence of each abbreviation\n",
    "abbreviations_unique = abbreviations.groupby('term', as_index=False).nth(0)\n",
    "\n",
    "# Filter out abbreviations that are already in the SNOMED terms\n",
    "unique_terms = active_snomed_descriptions['term'].values\n",
    "abbreviation_fresh = abbreviations_unique[~abbreviations_unique['term'].isin(unique_terms)]\n",
    "\n",
    "# Combine the original descriptions with the new unique abbreviations\n",
    "snomed_active_withabbreviation = pd.concat([active_snomed_descriptions, abbreviation_fresh], ignore_index=True, sort=False)\n",
    "\n",
    "# Write the terms to a CSV file for normalization\n",
    "snomed_active_withabbreviation['term'].to_csv(\"./snomed_active_withabbreviation_onlyterms.csv\", index=False, sep=',')\n",
    "\n",
    "# Note: The next step involves using an external command line tool for normalization\n",
    "# See: https://lhncbc.nlm.nih.gov/LSG/Projects/lvg/current/docs/userDoc/tools/norm.html\n",
    "# Command: norm -i:snomed_active_withabbreviation_onlyterms.csv -o:snomed_active_withabbreviation_onlyterms_normalize.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cfbb71-ca17-4ce7-b33b-8b3a93d835e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the normalized SNOMED terms from the CSV file\n",
    "snomed_terms_normalized = pd.read_csv('./chrome_matching/snomed_active_withabbreviation_onlyterms_normalize.csv',\n",
    "                                      header=None, names=['label', 'label_normalized'], sep='|')\n",
    "\n",
    "# Drop the first row which contains the term/header entry and remove duplicates\n",
    "snomed_terms_normalized = snomed_terms_normalized.drop([0])\n",
    "snomed_terms_normalized = snomed_terms_normalized.drop_duplicates(subset=['label', 'label_normalized'])\n",
    "\n",
    "# Set the 'term' column as the index for the DataFrame\n",
    "snomed_active_withabbreviation = snomed_active_withabbreviation.set_index('term')\n",
    "\n",
    "# Group by 'term' and create a set of 'conceptId' for each term\n",
    "snomed_active_withabbreviation_uniqueset = snomed_active_withabbreviation.groupby('term')['conceptId'].apply(set)\n",
    "\n",
    "# Convert the grouped Series to a DataFrame\n",
    "snomed_active_withabbreviation_uniqueset_df = pd.DataFrame(snomed_active_withabbreviation_uniqueset)\n",
    "\n",
    "# Add a column to count the number of unique 'conceptId's for each term\n",
    "snomed_active_withabbreviation_uniqueset_df['num_ids'] = snomed_active_withabbreviation_uniqueset.apply(lambda x: len(x))\n",
    "\n",
    "# The vast majority of terms are mapped to a single SCTID. In the rare occasion that a term is mapped to 2 or more different SCTIDs,\n",
    "# the SCTID is picked according to its semantic tag as described in the PNAS paper by Kurvers et al.\n",
    "# If none of the SCTIDs has a semantic tag from the 7 listed below, the SCTID is randomly chosen among the candidates.\n",
    "ordered_tags = ['disorder', 'finding', 'morphologic abnormality', 'body structure', 'person', 'organism', 'specimen']\n",
    "ordered_tags_dict = {k: i for i, k in enumerate(ordered_tags)}\n",
    "\n",
    "# Function to pick a single SCTID from a set of SCTIDs based on the preferred semantic tag\n",
    "def pick_single_sctid_from_set(sct_set):\n",
    "    sct_list = list(sct_set)\n",
    "    tags = [sctid_to_tag[sctid] if sctid in sctid_to_tag else '-' for sctid in sct_list]\n",
    "    tags_num = [ordered_tags_dict.get(tag, np.inf) for tag in tags]\n",
    "    if any(tag_num != np.inf for tag_num in tags_num):\n",
    "        return sct_list[np.argmin(tags_num)]\n",
    "    else:\n",
    "        return np.random.choice(sct_list)\n",
    "\n",
    "# Apply the function to pick a single SCTID for each term and add it as a new column\n",
    "snomed_active_withabbreviation_uniqueset_df['single_sct_id'] = snomed_active_withabbreviation_uniqueset_df[\"conceptId\"].map(pick_single_sctid_from_set)\n",
    "\n",
    "# Add the 'conceptId' back to the DataFrame by mapping the labels to their corresponding 'conceptId'\n",
    "def label_to_concept(label):\n",
    "    try:\n",
    "        return snomed_active_withabbreviation_uniqueset.loc[label]\n",
    "    except:\n",
    "        print('did not find ', label)\n",
    "        return set()\n",
    "\n",
    "# Map each label to its 'conceptId'\n",
    "snomed_terms_normalized['conceptId'] = snomed_terms_normalized['label'].map(label_to_concept)\n",
    "\n",
    "# Group by 'label' and 'label_normalized' and create sets of normalized labels and 'conceptId's\n",
    "label_to_normalized_df = snomed_terms_normalized.groupby('label')['label_normalized'].apply(set)\n",
    "normalized_to_sctid_df = snomed_terms_normalized.groupby('label_normalized')['conceptId'].apply(lambda x: set.union(*x))\n",
    "\n",
    "# Convert the grouped Series to dictionaries\n",
    "label_to_normalized = label_to_normalized_df.to_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbb827f-4f4c-43b6-85cd-1fb1f532406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load case and solve data\n",
    "cases_df = pd.read_csv('data/case_data.csv')\n",
    "cases_df['diagnosis_names'] = cases_df['diagnosis_names'].apply(ast.literal_eval)\n",
    "solves_df = pd.read_csv('data/solve_data.csv')\n",
    "solves_df['final_dxs'] = solves_df['final_dxs'].apply(ast.literal_eval)\n",
    "\n",
    "# List of pretext blocks to be removed from LLM responses\n",
    "RESPONSE_PRETEXT_BLOCKLIST = [\n",
    "    \"Sure, \",\n",
    "    \"Here is the \",\n",
    "    \"Here are \",\n",
    "    \"### Response:\",\n",
    "    \"The probable\",\n",
    "    \"The differential\",\n",
    "    \"The most probable\",\n",
    "]\n",
    "\n",
    "# Function to create a defaultdict of dicts\n",
    "def create_dict_defaultdict():\n",
    "    return defaultdict(dict)\n",
    "\n",
    "# Function to clean the raw responses from LLMs\n",
    "def _clean_response(response):\n",
    "    # Strip leading/trailing whitespace\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Remove pretext blocks from the beginning of the response\n",
    "    for blocklist_item in RESPONSE_PRETEXT_BLOCKLIST:\n",
    "        if response.startswith(blocklist_item):\n",
    "            response = \"\\n\".join(response.split(\"\\n\")[1:])\n",
    "            break\n",
    "\n",
    "    # Process each diagnosis in the response\n",
    "    responses = [\n",
    "        re.sub(\"^(?:-|\\*|â€¢|\\d+\\.)\", \"\", dx.strip()).strip()\n",
    "        for dx in response.split(\"\\n\")\n",
    "    ]\n",
    "    \n",
    "    # Remove various unwanted patterns from the responses\n",
    "    responses = [re.sub(\"(\\(SNOMED CT:.+)\\)\", \"\", dx.strip()).strip() for dx in responses]\n",
    "    responses = [re.sub(r'\\bSNOMED CT:\\s*[\\d|#]*\\s*[|\\-()]*\\s*', \"\", dx.strip()).strip() for dx in responses]\n",
    "    responses = [re.sub(\"(\\(\\d+)\\)\", \"\", dx.strip()).strip() for dx in responses]\n",
    "    responses = [re.sub(\"(\\([A-Z]\\d+.+)\\)\", \"\", dx.strip()).strip() for dx in responses]\n",
    "    responses = [re.sub(\"(- \\d\\d\\d+)\", \"\", dx.strip()).strip() for dx in responses]\n",
    "    \n",
    "    # Filter out empty responses\n",
    "    responses = [res for res in responses if res != '']\n",
    "    \n",
    "    # Further clean responses by splitting at certain characters if the length exceeds 6 words\n",
    "    responses = [res.split(':')[0] if len(res.split()) > 6 else res for res in responses]\n",
    "    responses = [res.split(' - ')[0] if len(res.split()) > 6 else res for res in responses]\n",
    "    \n",
    "    # Remove vertical bars and strip leading/trailing whitespace\n",
    "    responses = [dx.replace('|', '').strip() for dx in responses]\n",
    "    \n",
    "    # Return the first 5 cleaned responses\n",
    "    return responses[:5]\n",
    "\n",
    "# Initialize an empty dictionary to store LLM data\n",
    "llm_data = {}\n",
    "\n",
    "# Define keys for LLM models and corresponding saved keys\n",
    "mkeys = ['OpenAI_gpt-4-1106-preview', 'GoogleAI_gemini-1.0-pro', 'AnthropicAI_claude-3-opus-20240229','MetaAI_llama-2-70b-f','MistralAI_mistral-large-latest']\n",
    "saved_mkeys = ['OPENAI', 'GOOGLEAI', 'ANTHROPICAI', 'METAAI', 'MISTRALAI']\n",
    "\n",
    "# Load LLM responses from pickle files\n",
    "for other_key, key in zip(mkeys, saved_mkeys):\n",
    "    llm_data[other_key] = pickle.load(open(f\"./data/_data_machine_solves_varied_prompts_2024_03_29_{key}.pkl\", \"rb\"))\n",
    "\n",
    "# Extract all case IDs and experimental settings from the loaded data\n",
    "all_case_ids = list(llm_data[mkeys[0]].keys())\n",
    "exp_settings = list(llm_data[mkeys[0]][1883][mkeys[0]].keys())\n",
    "\n",
    "# Initialize an empty list to store machine data\n",
    "machine_data = []\n",
    "\n",
    "# Process each LLM model, case ID, and experimental setting\n",
    "for mkey in mkeys:\n",
    "    for cid in all_case_ids:\n",
    "        for exp in exp_settings:\n",
    "            dialist = _clean_response(llm_data[mkey][cid][mkey][exp][0])\n",
    "            if len(dialist) > 0:\n",
    "                for i, dia in enumerate(dialist):\n",
    "                    machine_data.append({\n",
    "                        'llm_model': mkey,\n",
    "                        'pc_id': cid,\n",
    "                        'prompt': exp,\n",
    "                        'diagnosis': dia,\n",
    "                        'rank': i + 1\n",
    "                    })\n",
    "            else:\n",
    "                machine_data.append({\n",
    "                    'llm_model': mkey,\n",
    "                    'pc_id': cid,\n",
    "                    'prompt': exp,\n",
    "                    'diagnosis': '',\n",
    "                    'rank': -1\n",
    "                })\n",
    "\n",
    "# Convert the list of machine data to a DataFrame\n",
    "machine_data_df = pd.DataFrame(machine_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17daf200-0e48-435f-b507-8dd46a69ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to collect all terms for normalization\n",
    "term_list = []\n",
    "\n",
    "# Extend the term list with diagnosis names from cases_df\n",
    "for diagnoses in cases_df['diagnosis_names'].values:\n",
    "    term_list.extend(diagnoses)\n",
    "\n",
    "# Extend the term list with final diagnoses from solves_df\n",
    "for diagnoses in solves_df['final_dxs'].values:\n",
    "    term_list.extend(diagnoses)\n",
    "\n",
    "# Extend the term list with diagnoses from machine_data_df\n",
    "for diagnoses in machine_data_df['diagnosis'].values:\n",
    "    term_list.extend([diagnoses])\n",
    "\n",
    "# Remove duplicate terms by converting the list to a set and then back to a list\n",
    "term_list = list(set(term_list))\n",
    "\n",
    "# Write the unique terms to a CSV file for normalization\n",
    "with open('data/terms_to_normalize.csv', 'w') as f:\n",
    "    for line in term_list:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "# Normalize terms via command line using the 'norm' tool (much quicker than individual normalization)\n",
    "# Command to normalize terms:\n",
    "# cmd = \"norm i:./data/terms_to_normalize.csv o:./data/terms_normalized.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b19fb69-5433-4a29-bba1-f626e4d53dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the normalized terms from the CSV file\n",
    "solves_terms_normalized = pd.read_csv('data/terms_normalized.csv', header=None, names=['label', 'label_normalized'], sep='|')\n",
    "\n",
    "# Drop duplicate entries to ensure unique mappings\n",
    "solves_terms_normalized = solves_terms_normalized.drop_duplicates(subset=['label', 'label_normalized'])\n",
    "\n",
    "# Group by the original label to collect all normalized versions\n",
    "solves_terms_normalized = solves_terms_normalized.groupby('label')['label_normalized'].apply(set)\n",
    "\n",
    "# Convert the grouped data into a dictionary for easy lookup\n",
    "term_to_normalized = solves_terms_normalized.to_dict(defaultdict(set))\n",
    "\n",
    "# Function to normalize diagnosis terms using an external normalization tool\n",
    "def normalize_diagnosis(raw, term_to_normalized={}):\n",
    "    existing = term_to_normalized.get(raw, None)\n",
    "    if existing is not None:\n",
    "        return existing\n",
    "    else:\n",
    "        # For a large number of raw labels, it is much faster to apply normalization to a file\n",
    "        cmd = f'echo \"{raw}\" | norm'\n",
    "        ps = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, executable=\"/bin/bash\", env=dict(PATH=\"~/hacid/lvg2024/bin/\"))\n",
    "        output = ps.communicate()[0].decode('UTF-8')\n",
    "        normalized = [x.split(\"|\")[-1] for x in output.split('\\n')[:-1]]\n",
    "        term_to_normalized[raw] = normalized\n",
    "        return normalized\n",
    "\n",
    "# Use memoization to cache results of the crome_matching function\n",
    "@lru_cache(maxsize=1000000)  # Set a cache size limit to avoid memory overuse\n",
    "def crome_matching(text, normalized_to_sctid=normalized_to_sctid):\n",
    "    normalized_items = normalize_diagnosis(text, term_to_normalized=term_to_normalized)\n",
    "    results = [normalized_to_sctid.get(item, None) for item in normalized_items]\n",
    "\n",
    "    # Flatten results and remove None\n",
    "    flat_results = [\n",
    "        element \n",
    "        for item in results \n",
    "        for element in (item if isinstance(item, (set, list)) else [item]) \n",
    "        if element is not None\n",
    "    ]\n",
    "\n",
    "    # Ensure uniqueness using a set\n",
    "    unique_results = list(set(flat_results))\n",
    "\n",
    "    if unique_results:\n",
    "        if len(unique_results) == 1:\n",
    "            return unique_results[0]\n",
    "        else:\n",
    "            # Choose the SCTID based on the ordered tags, prioritizing certain tags\n",
    "            tags = [sctid_to_tag[sctid] for sctid in unique_results]\n",
    "            tags_num = [ordered_tags_dict.get(tag, np.inf) for tag in tags]\n",
    "            \n",
    "            if any(tag_num != np.inf for tag_num in tags_num):\n",
    "                return unique_results[np.argmin(tags_num)]\n",
    "            else:\n",
    "                return np.random.choice(unique_results)\n",
    "    return None\n",
    "\n",
    "# Function to return all indices where the maximum value occurs\n",
    "def all_argmax(b):\n",
    "    return np.flatnonzero(b == b.max())\n",
    "\n",
    "### Matching\n",
    "# Set the TOKENIZERS_PARALLELISM to false before importing or using the tokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# load sentence transformer model\n",
    "sbiobertmodel = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO', device='cuda')\n",
    "\n",
    "# encode all unique snomed terms (including all synonyms) and abbreviation and create embeddings\n",
    "# this takes pretty long\n",
    "sct_embeddings = sbiobertmodel.encode(snomed_active_withabbreviation_uniqueset_df.index.values)\n",
    "\n",
    "# Use memoization to cache results of the str_to_sctid function\n",
    "@lru_cache(maxsize=1000000)  # Cache results to improve performance\n",
    "def str_to_sctid(text):\n",
    "    if text == '' or text == '-':\n",
    "        return None\n",
    "    result = crome_matching(text)  # Crome method -> see Kurvers et.al PNAS\n",
    "    if result is None:\n",
    "        # Use SBERT model to find the closest SCTID if Crome matching fails\n",
    "        diagnosis_emb = sbiobertmodel.encode(text)\n",
    "        cos_sims = util.cos_sim(diagnosis_emb, sct_embeddings).numpy()\n",
    "        results = snomed_active_withabbreviation_uniqueset_df.iloc[all_argmax(cos_sims)]['single_sct_id'].values\n",
    "        if len(results) == 1:\n",
    "            result = results[0]\n",
    "        else:\n",
    "            result = np.random.choice(results)\n",
    "    return result\n",
    "\n",
    "# Convert a list of strings to a set of SCTIDs\n",
    "def strs_to_sctid_set(text_iter):\n",
    "    return set([str_to_sctid(text) for text in text_iter])\n",
    "\n",
    "#match correct case diagnoses to snomed ids\n",
    "cases_df['sctids']= cases_df['diagnosis_names'].apply(strs_to_sctid_set)\n",
    "cases_df.to_csv('data/case_data_matched.csv', index=False)\n",
    "cases_df = cases_df.set_index('id')\n",
    "\n",
    "\n",
    "def is_correct(row):\n",
    "    return row['sctid'] in cases_df.loc[row['pc_id']].sctids\n",
    "\n",
    "#match all solve diagnoses to snomed ids\n",
    "solves_df_expanded= solves_df.explode('final_dxs').rename(columns={'final_dxs': 'diagnosis'})\n",
    "solves_df_expanded['rank'] = solves_df_expanded.groupby(level=0).cumcount() + 1 \n",
    "solves_df_expanded['sctid'] = solves_df_expanded['diagnosis'].apply(str_to_sctid)\n",
    "solves_df_expanded['sctid'] = solves_df_expanded['sctid'].astype('Int64')\n",
    "solves_df_expanded['is_correct'] = solves_df_expanded.apply(is_correct, axis=1)\n",
    "solves_df_expanded.to_csv('data/solves_data_matched.csv', index=False)\n",
    "\n",
    "#match all LLM solves to snomed ids\n",
    "machine_data_df['sctid'] = machine_data_df['diagnosis'].apply(str_to_sctid)\n",
    "machine_data_df['sctid'] = machine_data_df['sctid'].astype('Int64')\n",
    "machine_data_df = machine_data_df[machine_data_df['pc_id'].isin(cases_df.index)]\n",
    "machine_data_df['is_correct'] = machine_data_df.apply(is_correct, axis=1)\n",
    "machine_data_df.to_csv('data_to_share/llm_data_matched_validcases.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "hugging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
